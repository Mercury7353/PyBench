{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|system|>\n\nYou are an AI Agent who is proficient in solve complicated task. \nEach step you should wirte executable code to fulfill user query. Any Response without code means the task is completed and you do not have another chance to submit code\n\nYou are equipped with a codeinterpreter. You can give the code and get the execution result of your code. You should use the codeinterpreter in the following format: \n<|execute_start|>\n```python  \n\n<your code>  \n\n``` \n<|execute_end|> \n\n\nWARNING:Do not use cv2.waitKey(0) cv2.destroyAllWindows()!!! Or the program will be destoried\n\nEach round, your answer should ALWAYS use the following format(Each of your response should contain code, until you complete the task):\n\n\nAnalyse:(Analyse the message you received and plan what you should do)  \n\nThis Step Todo: One Subtask need to be done at this step  \n\nCode(WARNING:MAKE SURE YOU CODE FOLLOW THE FORMAT AND WRITE CODE OR THE TASK WILL BE FAILED): \n<|execute_start|>\n```python  \n\n<your code>  \n\n\n```  \n<|execute_end|>\n\n\nYou will got the result of your code after each step. When the code of previous subtask is excuted successfully, you can write and excuet the code for next subtask\nWhen all the code your write are executed and you got the code result that can fulfill the user query, you should summarize the previous analyse process and make a formal response to user, The response should follow this format:\nWARNING:MAKE SURE YOU GET THE CODE EXECUTED RESULT THAT FULFILLED ALL REQUIREMENT OF USER BEFORE USE \"Finished\"\nFinished: <Answer to user query>\n\nSome notice:\n1. When you want to draw a plot, use plt.savefig() and print the image path in markdown format instead of plt.show()\n2. Save anything to ./output folder\n3. End the process whenever you complete the task, When you do not have Action(Code), Use: Finished: <summary the analyse process and make response>\n4. Do not ask for user input in your python code.  \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|user|>\n\n[INFO]The data is uploaded to ./data/WizardMath_Paper.pdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|user|>\n\nsummarize this paper, and what is the main contributions of this paper"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The paper is uploaded, and I need to summarize it and identify the main contributions.\n\nThis Step Todo: Summarize the paper and identify the main contributions.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "There are some errors in the code. All variable in this cell should be redefined,Please Debug:\nError: DeprecationError\nPdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\nwith open('./data/WizardMath_Paper.pdf', 'rb') as f:\n    pdf = PyPDF2.PdfFileReader(f)\n    text = ''\n    for page in pdf.pages:\n        text += page.extractText()\n\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The code has an error due to the deprecation of PdfFileReader in PyPDF2 3.0.0. I need to redefine the variable to use PdfReader instead.\n\nThis Step Todo: Debug the code and redefine the variable.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WizardMath : Empowering Mathematical Reasoning\nfor Large Language Models via\nReinforced Evol-Instruct\nHaipeng Luo2\u21e4Qingfeng Sun1\u21e4Can Xu1\u2020Pu Zhao1Jianguang Lou1\nChongyang Tao1Xiubo Geng1Qingwei Lin1Shifeng Chen2\u2020Dongmei Zhang1\n1Microsoft\n2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n{caxu,qins,puzhao,jlou,chotao,xigeng,qlin,dongmeiz}@microsoft.com\n{hp.luo,shifeng.chen}@siat.ac.cn\nAbstract\nLarge language models (LLMs), such as GPT-4, have shown remarkable per-\nformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath , which enhances the mathematical rea-\nsoning abilities of Llama-2, by applying our proposed Reinforced Evol-Instruct\nmethod to the domain of math. Through extensive experiments on two mathe-\nmatical reasoning benchmarks, namely GSM8k and MATH, we reveal the ex-\ntraordinary capabilities of our model. WizardMath surpasses all other open-\nsource LLMs by a substantial margin. Furthermore, our model even outperforms\nChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously\nsurpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and\nmodel weights are public at https://github.com/nlpxucan/WizardLM3and\nhttps://huggingface.co/WizardLM .\n1 Introduction\nRecently, Large-scale language models (LLMs) have garnered signi\ufb01cant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further \ufb01ne-\ntuning with speci\ufb01c instruction data and methods. As a result, it achieves state-of-the-art zero-shot\nperformance on various benchmarks. Subsequently, Anthropic, Google, and Meta also launched\ntheir competitive products one after another. Notably, Meta\u2019s series of Llama [ 4,20] models have\nsparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs.\nThis trend also gradually stimulates the releases of MPT8, Falcon [ 21], StarCoder [ 12], Alpaca [ 22],\nVicuna [ 23], and WizardLM [ 24], etc. However, these open models still struggles with the scenarios\nwhich require complex multi-step quantitative reasoning, such as solving mathematical and science\nchallenges [25\u201335].\n\u21e4Equal contribution. Work done during the internship of Luo at Microsoft Research.\n\u2020Corresponding author: caxu@microsoft.com and shifeng.chen@siat.ac.cn\n3We are working with our legal team to review and publicly release the code and data in accordance with\nour policy.\nPreprint. Under review.SFTACBD\nC > A > B = DWizard-EChatGPTPPO\nIRMPRMC > A > B = DIRMPRM\ud835\udc5f\ud835\udc58\ud835\udc3c\ud835\udc5f\ud835\udc58\ud835\udc34\ud835\udc5f\ud835\udc58=\ud835\udc5f\ud835\udc58\ud835\udc3c\u2219\ud835\udc5f\ud835\udc58\ud835\udc34Wizard-EChatGPTWizard-EStep 1:Supervised fine-tuning.Step 2:Training Instruction Reward Model (IRM), and Process-supervised Reward Model (PRM).Step 3:Active Evol-Instruct, and PPO training.WizardLM\ud835\udefc Figure 1: A diagram illustrating the three steps of our method: (1) supervised \ufb01ne-tuning (SFT), (2)\nInstruction Reward Model (IRM) training and Process-supervised Reward Model (PRM) training,\nand (3) Active Evol-Instruct and reinforcement learning via proximal policy optimization (PPO).\nChain-of-thought (CoT) [ 31] proposes to design better prompts to generate step-by-step solutions,\nwhich can lead to improved performance. Self-Consistency [ 34] also achieves remarkable perfor-\nmance on many reasoning benchmarks, which generates several possible answers from the model\nand selects the correct one based on majority vote [ 35]. In recent, [ 36] \ufb01nds that process supervision\nwith reinforcement learning signi\ufb01cantly outperforms outcome supervision for solving challenging\nMATH problems.\nInspired by Evol-Instruct and Process-supervised Reinforcement Learning, this work aims to enhance\nthe mathematical reasoning abilities of the SOTA open-source LLM, Llama-2 [ 20]. As shown in the\nFigure 1, we propose a new method named Reinforced Evol-Instruct , which could \ufb01rstly generate\ndiverse math instructions data by math-speci\ufb01c Evol-Instruct , then we train an instruction reward\nmodel (IRM) and a process-supervised reward model (PRM) [ 16,36\u201341], the former indicates the\nquality of the evolved instruction and the later receives feedback for each step in the solution. The\nbrand-new Evol-Instruct method includes two downward evolution and upward evolution progress to\nproduce the grade school math and challenging math respectively. Initially, we re-generate, \ufb01lter and\n\ufb01netune the original math instruction data from GSM8k [ 42] and MATH [ 43]. Immediately, we train\nthe Llama-2 models to obtain the reward models and our WizardMath .\nWe perform experiments on two mathematical reasoning benchmarks, namely GSM8k [ 42] and\nMATH [ 43], the results demonstrate that our WizardMath outperforms all other open-source LLMs,\nachieving state-of-the-art performance. Speci\ufb01cally, WizardMath observe a substantial improvement\nin pass@1 with an increase of +24.8 (81.6. vs. 56.8) on GSM8k, and +9.2 (22.7 vs. 13.5) on MATH.\nNotably, our model even also signi\ufb01cantly surpasses OpenAI\u2019s ChatGPT-3.55, Anthropic\u2019s Claude\nInstant-1 [39], and Google\u2019s PaLM-2 [44] in terms of pass@1 on GSM8k.\nThe main contributions of this work are as following:\n\u2022We introduce WizardMath model, which enhances the mathematical reasoning abilities for\nopen-source pretrained large language model Llama-2 [20].\n2\u2022We propose a new method, Reinforced Evol-Instruct , alongside Evol-Instruct and Reinforce-\nment Learning, for improving LLM reasoning performance.\n\u2022WizardMath surpasses all other open-source LLMs by a substantial margin in terms of math-\nematical reasoning, including Llama-2 70B [ 20], Llama-1 65B [ 4], Falcon-40B [ 21], MPT-\n30B8, Baichuan-13B Chat9and ChatGLM2 12B [ 45] on both GSM8k [ 42] and MATH [ 43].\n\u2022WizardMath signi\ufb01cantly outperforms various main closed-source LLMs, such as ChatGPT5,\nGPT-3.5, Claude Instant [39], PaLM-2 [44], PaLM-1 [7] and Minerva[15] on GSM8k.\n2 Method\nIn this section, we elaborate on the details of our WizardMath . Following WizardLM and PRMs[ 36],\nwe propose Reinforced Evol-Instruct , which integrates the Evol-Instruct and reinforced process\nsupervision method to evolve GSM8k and MATH, and \ufb01ne-tune the pre-trained Llama-2 with the\nevolved data and reward models.\nAs shown in the Figure 1, our methods apply three steps:\n1.Supervised \ufb01ne-tuning.\n2.Training instruction reward model, and process-supervised reward model.\n3.Active Evol-Instruct, and PPO training.\n2.1 Supervised \ufb01ne-tuning\nFollowing InstructGPT[ 2], we also \ufb01rstly \ufb01ne tune the base with supervised instruction-response\npairs, which contains:\n1.To make the parsing of each step easier, we few-shot re-generate 15k answers for GSM8k\nand MATH with an Alpha version of WizardLM 70B model to produce solutions in a\nstep-by-step format, then \ufb01nd out those with a correct answer, and use this data to \ufb01netune\nbase Llama model.\n2.To enhance the model\u2019s ability to adhere to the neural and diverse instructions, we also\nsample 1.5k open-domain conversations from WizardLM\u2019s training data, then merge it with\nabove math corpus as the \ufb01nal SFT training data.\n2.2 Evol-Instruct principles for math\nMotivated by the Evol-Instruct [ 24] method proposed by WiazrdLM and its effective application\non WizardCoder [ 13], this work attempts to make math instructions with various complexities and\ndiversity to enhance the pre-trained LLMs. Speci\ufb01cally, we adapt Evol-Instruct to a new paradigm\nincluding two evolution lines:\n1.Downward evolution: It enhances instructions by making the questions easier. For example\ni): revising high dif\ufb01culty questions to lower dif\ufb01culty, or ii) producing a new and easier\nquestion with another different topic.\n2.Upward evolution: Derived from original Evol-Instruct method, it deepens and generates\nnew and harder questions by i) adding more constraints, ii) concretizing, iii) increasing\nreasoning.\n2.3 Reinforced Evol-Instruct\nInspired by InstructGPT[ 2] and PRMs[ 36], we train two reward models to predict the quality of the\ninstructions and the correctness of each step in the answer respectively:\n1.Instruction Reward Model (IRM): This model aims to judge the quality of the evolved\ninstructions on three aspects: i) De\ufb01nition, ii) Precision, and iii) Integrity. To produce\nthe ranking list training data of IRM, for each instruction, we \ufb01rstly use ChatGPT and\n3\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\nwith open('./data/WizardMath_Paper.pdf', 'rb') as f:\n    pdf = PyPDF2.PdfReader(f)\n    text = ''\n    for page in pdf.pages:\n        text += page.extract_text()\n\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The paper discusses the WizardMath model, which enhances the mathematical reasoning abilities of large language models (LLMs) using a new method called Reinforced Evol-Instruct. The paper presents the main contributions of the work, which include introducing the WizardMath model, proposing the Reinforced Evol-Instruct method, and achieving state-of-the-art performance on two mathematical reasoning benchmarks.\n\nThis Step Todo: Summarize the main contributions of the paper.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WizardMath : Empowering Mathematical Reasoning\nfor Large Language Models via\nReinforced Evol-Instruct\nHaipeng Luo2\u21e4Qingfeng Sun1\u21e4Can Xu1\u2020Pu Zhao1Jianguang Lou1\nChongyang Tao1Xiubo Geng1Qingwei Lin1Shifeng Chen2\u2020Dongmei Zhang1\n1Microsoft\n2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n{caxu,qins,puzhao,jlou,chotao,xigeng,qlin,dongmeiz}@microsoft.com\n{hp.luo,shifeng.chen}@siat.ac.cn\nAbstract\nLarge language models (LLMs), such as GPT-4, have shown remarkable per-\nformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath , which enhances the mathematical rea-\nsoning abilities of Llama-2, by applying our proposed Reinforced Evol-Instruct\nmethod to the domain of math. Through extensive experiments on two mathe-\nmatical reasoning benchmarks, namely GSM8k and MATH, we reveal the ex-\ntraordinary capabilities of our model. WizardMath surpasses all other open-\nsource LLMs by a substantial margin. Furthermore, our model even outperforms\nChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously\nsurpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and\nmodel weights are public at https://github.com/nlpxucan/WizardLM3and\nhttps://huggingface.co/WizardLM .\n1 Introduction\nRecently, Large-scale language models (LLMs) have garnered signi\ufb01cant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further \ufb01ne-\ntuning with speci\ufb01c instruction data and methods. As a result, it achieves state-of-the-art zero-shot\nperformance on various benchmarks. Subsequently, Anthropic, Google, and Meta also launched\ntheir competitive products one after another. Notably, Meta\u2019s series of Llama [ 4,20] models have\nsparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs.\nThis trend also gradually stimulates the releases of MPT8, Falcon [ 21], StarCoder [ 12], Alpaca [ 22],\nVicuna [ 23], and WizardLM [ 24], etc. However, these open models still struggles with the scenarios\nwhich require complex multi-step quantitative reasoning, such as solving mathematical and science\nchallenges [25\u201335].\n\u21e4Equal contribution. Work done during the internship of Luo at Microsoft Research.\n\u2020Corresponding author: caxu@microsoft.com and shifeng.chen@siat.ac.cn\n3We are working with our legal team to review and publicly release the code and data in accordance with\nour policy.\nPreprint. Under review.SFTACBD\nC > A > B = DWizard-EChatGPTPPO\nIRMPRMC > A > B = DIRMPRM\ud835\udc5f\ud835\udc58\ud835\udc3c\ud835\udc5f\ud835\udc58\ud835\udc34\ud835\udc5f\ud835\udc58=\ud835\udc5f\ud835\udc58\ud835\udc3c\u2219\ud835\udc5f\ud835\udc58\ud835\udc34Wizard-EChatGPTWizard-EStep 1:Supervised fine-tuning.Step 2:Training Instruction Reward Model (IRM), and Process-supervised Reward Model (PRM).Step 3:Active Evol-Instruct, and PPO training.WizardLM\ud835\udefc Figure 1: A diagram illustrating the three steps of our method: (1) supervised \ufb01ne-tuning (SFT), (2)\nInstruction Reward Model (IRM) training and Process-supervised Reward Model (PRM) training,\nand (3) Active Evol-Instruct and reinforcement learning via proximal policy optimization (PPO).\nChain-of-thought (CoT) [ 31] proposes to design better prompts to generate step-by-step solutions,\nwhich can lead to improved performance. Self-Consistency [ 34] also achieves remarkable perfor-\nmance on many reasoning benchmarks, which generates several possible answers from the model\nand selects the correct one based on majority vote [ 35]. In recent, [ 36] \ufb01nds that process supervision\nwith reinforcement learning signi\ufb01cantly outperforms outcome supervision for solving challenging\nMATH problems.\nInspired by Evol-Instruct and Process-supervised Reinforcement Learning, this work aims to enhance\nthe mathematical reasoning abilities of the SOTA open-source LLM, Llama-2 [ 20]. As shown in the\nFigure 1, we propose a new method named Reinforced Evol-Instruct , which could \ufb01rstly generate\ndiverse math instructions data by math-speci\ufb01c Evol-Instruct , then we train an instruction reward\nmodel (IRM) and a process-supervised reward model (PRM) [ 16,36\u201341], the former indicates the\nquality of the evolved instruction and the later receives feedback for each step in the solution. The\nbrand-new Evol-Instruct method includes two downward evolution and upward evolution progress to\nproduce the grade school math and challenging math respectively. Initially, we re-generate, \ufb01lter and\n\ufb01netune the original math instruction data from GSM8k [ 42] and MATH [ 43]. Immediately, we train\nthe Llama-2 models to obtain the reward models and our WizardMath .\nWe perform experiments on two mathematical reasoning benchmarks, namely GSM8k [ 42] and\nMATH [ 43], the results demonstrate that our WizardMath outperforms all other open-source LLMs,\nachieving state-of-the-art performance. Speci\ufb01cally, WizardMath observe a substantial improvement\nin pass@1 with an increase of +24.8 (81.6. vs. 56.8) on GSM8k, and +9.2 (22.7 vs. 13.5) on MATH.\nNotably, our model even also signi\ufb01cantly surpasses OpenAI\u2019s ChatGPT-3.55, Anthropic\u2019s Claude\nInstant-1 [39], and Google\u2019s PaLM-2 [44] in terms of pass@1 on GSM8k.\nThe main contributions of this work are as following:\n\u2022We introduce WizardMath model, which enhances the mathematical reasoning abilities for\nopen-source pretrained large language model Llama-2 [20].\n2\u2022We propose a new method, Reinforced Evol-Instruct , alongside Evol-Instruct and Reinforce-\nment Learning, for improving LLM reasoning performance.\n\u2022WizardMath surpasses all other open-source LLMs by a substantial margin in terms of math-\nematical reasoning, including Llama-2 70B [ 20], Llama-1 65B [ 4], Falcon-40B [ 21], MPT-\n30B8, Baichuan-13B Chat9and ChatGLM2 12B [ 45] on both GSM8k [ 42] and MATH [ 43].\n\u2022WizardMath signi\ufb01cantly outperforms various main closed-source LLMs, such as ChatGPT5,\nGPT-3.5, Claude Instant [39], PaLM-2 [44], PaLM-1 [7] and Minerva[15] on GSM8k.\n2 Method\nIn this section, we elaborate on the details of our WizardMath . Following WizardLM and PRMs[ 36],\nwe propose Reinforced Evol-Instruct , which integrates the Evol-Instruct and reinforced process\nsupervision method to evolve GSM8k and MATH, and \ufb01ne-tune the pre-trained Llama-2 with the\nevolved data and reward models.\nAs shown in the Figure 1, our methods apply three steps:\n1.Supervised \ufb01ne-tuning.\n2.Training instruction reward model, and process-supervised reward model.\n3.Active Evol-Instruct, and PPO training.\n2.1 Supervised \ufb01ne-tuning\nFollowing InstructGPT[ 2], we also \ufb01rstly \ufb01ne tune the base with supervised instruction-response\npairs, which contains:\n1.To make the parsing of each step easier, we few-shot re-generate 15k answers for GSM8k\nand MATH with an Alpha version of WizardLM 70B model to produce solutions in a\nstep-by-step format, then \ufb01nd out those with a correct answer, and use this data to \ufb01netune\nbase Llama model.\n2.To enhance the model\u2019s ability to adhere to the neural and diverse instructions, we also\nsample 1.5k open-domain conversations from WizardLM\u2019s training data, then merge it with\nabove math corpus as the \ufb01nal SFT training data.\n2.2 Evol-Instruct principles for math\nMotivated by the Evol-Instruct [ 24] method proposed by WiazrdLM and its effective application\non WizardCoder [ 13], this work attempts to make math instructions with various complexities and\ndiversity to enhance the pre-trained LLMs. Speci\ufb01cally, we adapt Evol-Instruct to a new paradigm\nincluding two evolution lines:\n1.Downward evolution: It enhances instructions by making the questions easier. For example\ni): revising high dif\ufb01culty questions to lower dif\ufb01culty, or ii) producing a new and easier\nquestion with another different topic.\n2.Upward evolution: Derived from original Evol-Instruct method, it deepens and generates\nnew and harder questions by i) adding more constraints, ii) concretizing, iii) increasing\nreasoning.\n2.3 Reinforced Evol-Instruct\nInspired by InstructGPT[ 2] and PRMs[ 36], we train two reward models to predict the quality of the\ninstructions and the correctness of each step in the answer respectively:\n1.Instruction Reward Model (IRM): This model aims to judge the quality of the evolved\ninstructions on three aspects: i) De\ufb01nition, ii) Precision, and iii) Integrity. To produce\nthe ranking list training data of IRM, for each instruction, we \ufb01rstly use ChatGPT and\n3\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\nwith open('./data/WizardMath_Paper.pdf', 'rb') as f:\n    pdf = PyPDF2.PdfReader(f)\n    text = ''\n    for page in pdf.pages:\n        text += page.extract_text()\n\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nThe main contributions of this paper are:\n\n1. Introducing the WizardMath model, which enhances the mathematical reasoning abilities of open-source pre-trained large language models (LLMs) such as Llama-2.\n2. Proposing a new method called Reinforced Evol-Instruct, which integrates Evol-Instruct and reinforced process supervision to evolve math instructions data and fine-tune the pre-trained LLM with the evolved data and reward models.\n3. Achieving state-of-the-art performance on two mathematical reasoning benchmarks, GSM8k and MATH, and surpassing all other open-source LLMs by a substantial margin.\n\nThe paper also presents the Reinforced Evol-Instruct method, which includes three steps: supervised fine-tuning, training instruction reward model and process-supervised reward model, and active Evol-Instruct and PPO training. The method uses two evolution lines, downward evolution and upward evolution, to generate diverse math instructions data and fine-tune the pre-trained LLM.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nAnalyse: The paper discusses the main contributions of the WizardMath model, which enhances the mathematical reasoning abilities of large language models (LLMs) using a new method called Reinforced Evol-Instruct.\n\nThis Step Todo: Summarize the main contributions of the paper.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WizardMath : Empowering Mathematical Reasoning\nfor Large Language Models via\nReinforced Evol-Instruct\nHaipeng Luo2\u21e4Qingfeng Sun1\u21e4Can Xu1\u2020Pu Zhao1Jianguang Lou1\nChongyang Tao1Xiubo Geng1Qingwei Lin1Shifeng Chen2\u2020Dongmei Zhang1\n1Microsoft\n2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n{caxu,qins,puzhao,jlou,chotao,xigeng,qlin,dongmeiz}@microsoft.com\n{hp.luo,shifeng.chen}@siat.ac.cn\nAbstract\nLarge language models (LLMs), such as GPT-4, have shown remarkable per-\nformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath , which enhances the mathematical rea-\nsoning abilities of Llama-2, by applying our proposed Reinforced Evol-Instruct\nmethod to the domain of math. Through extensive experiments on two mathe-\nmatical reasoning benchmarks, namely GSM8k and MATH, we reveal the ex-\ntraordinary capabilities of our model. WizardMath surpasses all other open-\nsource LLMs by a substantial margin. Furthermore, our model even outperforms\nChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously\nsurpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and\nmodel weights are public at https://github.com/nlpxucan/WizardLM3and\nhttps://huggingface.co/WizardLM .\n1 Introduction\nRecently, Large-scale language models (LLMs) have garnered signi\ufb01cant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further \ufb01ne-\ntuning with speci\ufb01c instruction data and methods. As a result, it achieves state-of-the-art zero-shot\nperformance on various benchmarks. Subsequently, Anthropic, Google, and Meta also launched\ntheir competitive products one after another. Notably, Meta\u2019s series of Llama [ 4,20] models have\nsparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs.\nThis trend also gradually stimulates the releases of MPT8, Falcon [ 21], StarCoder [ 12], Alpaca [ 22],\nVicuna [ 23], and WizardLM [ 24], etc. However, these open models still struggles with the scenarios\nwhich require complex multi-step quantitative reasoning, such as solving mathematical and science\nchallenges [25\u201335].\n\u21e4Equal contribution. Work done during the internship of Luo at Microsoft Research.\n\u2020Corresponding author: caxu@microsoft.com and shifeng.chen@siat.ac.cn\n3We are working with our legal team to review and publicly release the code and data in accordance with\nour policy.\nPreprint. Under review.SFTACBD\nC > A > B = DWizard-EChatGPTPPO\nIRMPRMC > A > B = DIRMPRM\ud835\udc5f\ud835\udc58\ud835\udc3c\ud835\udc5f\ud835\udc58\ud835\udc34\ud835\udc5f\ud835\udc58=\ud835\udc5f\ud835\udc58\ud835\udc3c\u2219\ud835\udc5f\ud835\udc58\ud835\udc34Wizard-EChatGPTWizard-EStep 1:Supervised fine-tuning.Step 2:Training Instruction Reward Model (IRM), and Process-supervised Reward Model (PRM).Step 3:Active Evol-Instruct, and PPO training.WizardLM\ud835\udefc Figure 1: A diagram illustrating the three steps of our method: (1) supervised \ufb01ne-tuning (SFT), (2)\nInstruction Reward Model (IRM) training and Process-supervised Reward Model (PRM) training,\nand (3) Active Evol-Instruct and reinforcement learning via proximal policy optimization (PPO).\nChain-of-thought (CoT) [ 31] proposes to design better prompts to generate step-by-step solutions,\nwhich can lead to improved performance. Self-Consistency [ 34] also achieves remarkable perfor-\nmance on many reasoning benchmarks, which generates several possible answers from the model\nand selects the correct one based on majority vote [ 35]. In recent, [ 36] \ufb01nds that process supervision\nwith reinforcement learning signi\ufb01cantly outperforms outcome supervision for solving challenging\nMATH problems.\nInspired by Evol-Instruct and Process-supervised Reinforcement Learning, this work aims to enhance\nthe mathematical reasoning abilities of the SOTA open-source LLM, Llama-2 [ 20]. As shown in the\nFigure 1, we propose a new method named Reinforced Evol-Instruct , which could \ufb01rstly generate\ndiverse math instructions data by math-speci\ufb01c Evol-Instruct , then we train an instruction reward\nmodel (IRM) and a process-supervised reward model (PRM) [ 16,36\u201341], the former indicates the\nquality of the evolved instruction and the later receives feedback for each step in the solution. The\nbrand-new Evol-Instruct method includes two downward evolution and upward evolution progress to\nproduce the grade school math and challenging math respectively. Initially, we re-generate, \ufb01lter and\n\ufb01netune the original math instruction data from GSM8k [ 42] and MATH [ 43]. Immediately, we train\nthe Llama-2 models to obtain the reward models and our WizardMath .\nWe perform experiments on two mathematical reasoning benchmarks, namely GSM8k [ 42] and\nMATH [ 43], the results demonstrate that our WizardMath outperforms all other open-source LLMs,\nachieving state-of-the-art performance. Speci\ufb01cally, WizardMath observe a substantial improvement\nin pass@1 with an increase of +24.8 (81.6. vs. 56.8) on GSM8k, and +9.2 (22.7 vs. 13.5) on MATH.\nNotably, our model even also signi\ufb01cantly surpasses OpenAI\u2019s ChatGPT-3.55, Anthropic\u2019s Claude\nInstant-1 [39], and Google\u2019s PaLM-2 [44] in terms of pass@1 on GSM8k.\nThe main contributions of this work are as following:\n\u2022We introduce WizardMath model, which enhances the mathematical reasoning abilities for\nopen-source pretrained large language model Llama-2 [20].\n2\u2022We propose a new method, Reinforced Evol-Instruct , alongside Evol-Instruct and Reinforce-\nment Learning, for improving LLM reasoning performance.\n\u2022WizardMath surpasses all other open-source LLMs by a substantial margin in terms of math-\nematical reasoning, including Llama-2 70B [ 20], Llama-1 65B [ 4], Falcon-40B [ 21], MPT-\n30B8, Baichuan-13B Chat9and ChatGLM2 12B [ 45] on both GSM8k [ 42] and MATH [ 43].\n\u2022WizardMath signi\ufb01cantly outperforms various main closed-source LLMs, such as ChatGPT5,\nGPT-3.5, Claude Instant [39], PaLM-2 [44], PaLM-1 [7] and Minerva[15] on GSM8k.\n2 Method\nIn this section, we elaborate on the details of our WizardMath . Following WizardLM and PRMs[ 36],\nwe propose Reinforced Evol-Instruct , which integrates the Evol-Instruct and reinforced process\nsupervision method to evolve GSM8k and MATH, and \ufb01ne-tune the pre-trained Llama-2 with the\nevolved data and reward models.\nAs shown in the Figure 1, our methods apply three steps:\n1.Supervised \ufb01ne-tuning.\n2.Training instruction reward model, and process-supervised reward model.\n3.Active Evol-Instruct, and PPO training.\n2.1 Supervised \ufb01ne-tuning\nFollowing InstructGPT[ 2], we also \ufb01rstly \ufb01ne tune the base with supervised instruction-response\npairs, which contains:\n1.To make the parsing of each step easier, we few-shot re-generate 15k answers for GSM8k\nand MATH with an Alpha version of WizardLM 70B model to produce solutions in a\nstep-by-step format, then \ufb01nd out those with a correct answer, and use this data to \ufb01netune\nbase Llama model.\n2.To enhance the model\u2019s ability to adhere to the neural and diverse instructions, we also\nsample 1.5k open-domain conversations from WizardLM\u2019s training data, then merge it with\nabove math corpus as the \ufb01nal SFT training data.\n2.2 Evol-Instruct principles for math\nMotivated by the Evol-Instruct [ 24] method proposed by WiazrdLM and its effective application\non WizardCoder [ 13], this work attempts to make math instructions with various complexities and\ndiversity to enhance the pre-trained LLMs. Speci\ufb01cally, we adapt Evol-Instruct to a new paradigm\nincluding two evolution lines:\n1.Downward evolution: It enhances instructions by making the questions easier. For example\ni): revising high dif\ufb01culty questions to lower dif\ufb01culty, or ii) producing a new and easier\nquestion with another different topic.\n2.Upward evolution: Derived from original Evol-Instruct method, it deepens and generates\nnew and harder questions by i) adding more constraints, ii) concretizing, iii) increasing\nreasoning.\n2.3 Reinforced Evol-Instruct\nInspired by InstructGPT[ 2] and PRMs[ 36], we train two reward models to predict the quality of the\ninstructions and the correctness of each step in the answer respectively:\n1.Instruction Reward Model (IRM): This model aims to judge the quality of the evolved\ninstructions on three aspects: i) De\ufb01nition, ii) Precision, and iii) Integrity. To produce\nthe ranking list training data of IRM, for each instruction, we \ufb01rstly use ChatGPT and\n3\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\nwith open('./data/WizardMath_Paper.pdf', 'rb') as f:\n    pdf = PyPDF2.PdfReader(f)\n    text = ''\n    for page in pdf.pages:\n        text += page.extract_text()\n\nprint(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}