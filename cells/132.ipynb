{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|system|>\n\nYou are an AI Agent who is proficient in solve complicated task. \nEach step you should wirte executable code to fulfill user query. Any Response without code means the task is completed and you do not have another chance to submit code\n\nYou are equipped with a codeinterpreter. You can give the code and get the execution result of your code. You should use the codeinterpreter in the following format: \n<|execute_start|>\n```python  \n\n<your code>  \n\n``` \n<|execute_end|> \n\n\nWARNING:Do not use cv2.waitKey(0) cv2.destroyAllWindows()!!! Or the program will be destoried\n\nEach round, your answer should ALWAYS use the following format(Each of your response should contain code, until you complete the task):\n\n\nAnalyse:(Analyse the message you received and plan what you should do)  \n\nThis Step Todo: One Subtask need to be done at this step  \n\nCode(WARNING:MAKE SURE YOU CODE FOLLOW THE FORMAT AND WRITE CODE OR THE TASK WILL BE FAILED): \n<|execute_start|>\n```python  \n\n<your code>  \n\n\n```  \n<|execute_end|>\n\n\nYou will got the result of your code after each step. When the code of previous subtask is excuted successfully, you can write and excuet the code for next subtask\nWhen all the code your write are executed and you got the code result that can fulfill the user query, you should summarize the previous analyse process and make a formal response to user, The response should follow this format:\nWARNING:MAKE SURE YOU GET THE CODE EXECUTED RESULT THAT FULFILLED ALL REQUIREMENT OF USER BEFORE USE \"Finished\"\nFinished: <Answer to user query>\n\nSome notice:\n1. When you want to draw a plot, use plt.savefig() and print the image path in markdown format instead of plt.show()\n2. Save anything to ./output folder\n3. End the process whenever you complete the task, When you do not have Action(Code), Use: Finished: <summary the analyse process and make response>\n4. Do not ask for user input in your python code.  \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|user|>\n\n[INFO]The data is uploaded to ./data/2401.02954v1.pdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|user|>\n\nHow much higher is the rating of the Deepseek model compared to Llama2 on Humaneval?"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The data uploaded is a PDF file, which contains the information about the paper \"2401.02954v1.pdf\". We need to extract the rating of the Deepseek model and Llama2 on Humaneval from the PDF file.\n\nThis Step Todo: Extract the rating information from the PDF file.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "There are some errors in the code. All variable in this cell should be redefined,Please Debug:\nError: SyntaxError\ninvalid syntax. Perhaps you forgot a comma? (1778937551.py, line 14)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\n# Open the PDF file\npdf_file = open('./data/2401.02954v1.pdf', 'rb')\npdf_reader = PyPDF2.PdfFileReader(pdf_file)\n\n# Extract the text from the PDF file\ntext = ''\nfor page in pdf_reader.pages:\n    text += page.extractText()\n\n# Find the rating information\nrating_deepseek = None\nrating_ llama2 = None\nfor line in text.split('\\n'):\n    if 'Deepseek' in line and 'Humaneval' in line:\n        rating_deepseek = float(line.split()[-1])\n    elif 'Llama2' in line and 'Humaneval' in line:\n        rating_ llama2 = float(line.split()[-1])\n\n# Calculate the difference in ratings\nif rating_deepseek is not None and rating_ llama2 is not None:\n    difference = rating_deepseek - rating_ llama2\n    print(f'The rating of the Deepseek model is {rating_deepseek} and the rating of Llama2 is {rating_ llama2}. The Deepseek model is {difference} higher than Llama2 on Humaneval.')\nelse:\n    print('Failed to extract the rating information.')\n\npdf_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The code has a syntax error on line 14. The error is likely due to the fact that I used two underscores before and after the variable names, which is not allowed in Python.\n\nThis Step Todo: Fix the syntax error by removing the underscores from the variable names.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "There are some errors in the code. All variable in this cell should be redefined,Please Debug:\nError: DeprecationError\nPdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\n# Open the PDF file\npdf_file = open('./data/2401.02954v1.pdf', 'rb')\npdf_reader = PyPDF2.PdfFileReader(pdf_file)\n\n# Extract the text from the PDF file\ntext = ''\nfor page in pdf_reader.pages:\n    text += page.extractText()\n\n# Find the rating information\nrating_deepseek = None\nrating_llama2 = None\nfor line in text.split('\\n'):\n    if 'Deepseek' in line and 'Humaneval' in line:\n        rating_deepseek = float(line.split()[-1])\n    elif 'Llama2' in line and 'Humaneval' in line:\n        rating_llama2 = float(line.split()[-1])\n\n# Calculate the difference in ratings\nif rating_deepseek is not None and rating_llama2 is not None:\n    difference = rating_deepseek - rating_llama2\n    print(f'The rating of the Deepseek model is {rating_deepseek} and the rating of Llama2 is {rating_llama2}. The Deepseek model is {difference} higher than Llama2 on Humaneval.')\nelse:\n    print('Failed to extract the rating information.')\n\npdf_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The error message indicates that the `PdfFileReader` class is deprecated and has been removed in PyPDF2 3.0.0. We need to use the `PdfReader` class instead.\n\nThis Step Todo: Update the code to use `PdfReader` instead of `PdfFileReader`.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Failed to extract the rating information.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\n# Open the PDF file\npdf_file = open('./data/2401.02954v1.pdf', 'rb')\npdf_reader = PyPDF2.PdfReader(pdf_file)\n\n# Extract the text from the PDF file\ntext = ''\nfor page in pdf_reader.pages:\n    text += page.extract_text()\n\n# Find the rating information\nrating_deepseek = None\nrating_llama2 = None\nfor line in text.split('\\n'):\n    if 'Deepseek' in line and 'Humaneval' in line:\n        rating_deepseek = float(line.split()[-1])\n    elif 'Llama2' in line and 'Humaneval' in line:\n        rating_llama2 = float(line.split()[-1])\n\n# Calculate the difference in ratings\nif rating_deepseek is not None and rating_llama2 is not None:\n    difference = rating_deepseek - rating_llama2\n    print(f'The rating of the Deepseek model is {rating_deepseek} and the rating of Llama2 is {rating_llama2}. The Deepseek model is {difference} higher than Llama2 on Humaneval.')\nelse:\n    print('Failed to extract the rating information.')\n\npdf_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The code was unable to extract the rating information from the PDF file. This could be due to several reasons such as the rating information not being present in the PDF file, or the code not being able to correctly identify the rating information.\n\nThis Step Todo: Debug the code to see why it was unable to extract the rating information.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DeepSeek LLM\nScaling Open-Source Language Models with Longtermism\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,\nRuiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He,\nWenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang,\nFangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu,\nShanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu,\nTongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song,\nXuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang,\nShiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie,\nYiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu,\nXingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang,\nMinghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,\nShangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou*\n*DeepSeek-AI\nAbstract\nThe rapid development of open-source large language models (LLMs) has been truly remarkable.\nHowever, the scaling laws described in previous literature presents varying conclusions, which\ncasts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our\ndistinctive \ufb01ndings that facilitate the scaling of large scale models in two prevalent used open-\nsource con\ufb01gurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM,\na project dedicated to advancing open-source language models with a long-term perspective.\nTo support the pre-training phase, we have developed a dataset that currently consists of 2\ntrillion tokens and is continuously expanding. We further conduct supervised \ufb01ne-tuning (SFT)\nand direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM\n67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code,\nmathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek\nLLM 67B Chat exhibits superior performance compared to GPT-3.5.\n*Authors are ordered alphabetically by the last name.arXiv:2401.02954v1  [cs.CL]  5 Jan 2024Contents\n1 Introduction 3\n2 Pre-Training 4\n2.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.4 Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3 Scaling Laws 7\n3.1 Scaling Laws for Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Estimating Optimal Model and Data Scaling . . . . . . . . . . . . . . . . . . . . . 9\n3.3 Scaling Laws with Different Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4 Alignment 12\n5 Evaluation 13\n5.1 Public Benchmark Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n5.1.1 Base Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n5.1.2 Chat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n5.2 Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2.1 Chinese Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2.2 English Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.3 Held-Out Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.4 Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n6 Conclusion, Limitation, and Future Work 23\nA Appendix 30\nA.1 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.2 Different Model Scale Representations . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.3 Benchmark Metrics Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA.4 Comparison with Code or Math Speci\ufb01c Models . . . . . . . . . . . . . . . . . . . 32\nA.5 Benchmark Results w/ DPO Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nA.6 Evaluation Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n21.Introduction\nOver the past few years, Large Language Models (LLMs) based on decoder-only Transformers\n(Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Arti-\n\ufb01cial General Intelligence (AGI). By predicting the next word in continuous text, LLMs undergo\nself-supervised pre-training on massive datasets, enabling them to achieve various purposes and\npossess many abilities, such as novel creation, text summarization, code completion, and more.\nSubsequent developments like supervised \ufb01ne-tuning and reward modeling have enabled Large\nLanguage Models (LLMs) to better follow user intentions and instructions. This has endowed\nthem with more versatile conversational capabilities and rapidly expanded their in\ufb02uence.\nThis wave is sparked with closed products , such as ChatGPT (OpenAI, 2022), Claude (An-\nthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational\nresources and substantial annotation costs. These products have signi\ufb01cantly raised the commu-\nnity\u2019s expectations for the capabilities of open-source LLMs, consequently inspiring a series of\nwork (Bai et al., 2023; Du et al., 2022; Jiang et al., 2023; Touvron et al., 2023a,b; Yang et al., 2023).\nAmong these, the LLaMA series models (Touvron et al., 2023a,b) stand out. It consolidates a\nrange of works to create an ef\ufb01cient and stable architecture, building well-performing models\nranging from 7B to 70B parameters. Consequently, the LLaMA series has become the de facto\nbenchmark for architecture and performance among open-source models.\nFollowing LLaMA, the open-source community has primarily focused on training \ufb01xed-size\n(7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM\nscaling laws (Hoffmann et al., 2022; Kaplan et al., 2020). Nonetheless, research on scaling laws is\nof utmost importance, considering that the current open-source models are merely at the initial\nstage of Arti\ufb01cial General Intelligence (AGI) development. In addition, early works (Hoffmann\net al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data\nwith increased compute budgets and inadequately addressed hyperparameter discussions. In\nthis paper, we extensively investigate the scaling behavior of language models and apply our\n\ufb01ndings in two widely used large-scale model con\ufb01gurations, namely 7B and 67B. Our study\naims to lay the groundwork for future scaling of open-source LLMs, paving the way for further\nadvancements in this domain. Speci\ufb01cally, we \ufb01rst examined the scaling laws of batch size\nand learning rate, and found their trends with model size. Building on this, we conducted a\ncomprehensive study of the scaling laws of the data and model scale, successfully revealing the\noptimal model/data scaling-up allocation strategy and predicting the expected performance\nof our large-scale models. Additionally, during development, we discovered that the scaling\nlaws derived from different datasets show signi\ufb01cant differences. This suggests that choice\nof dataset remarkably affects the scaling behavior, indicating that caution should be exercised\nwhen generalizing scaling laws across datasets.\nUnder the guidance of our scaling laws, we build from scratch open-source large language\nmodels, and release as much information as possible for community reference. We collect\n2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we\ngenerally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler\nwith a multi-step learning rate scheduler, maintaining performance while facilitating continual\ntraining. We collected over 1 million instances for supervised \ufb01ne-tuning (SFT) (Ouyang et al.,\n2022) from diverse sources. This paper shares our experiences with different SFT strategies\nand \ufb01ndings in data ablation techniques. Additionally, we have utilized direct preference\noptimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the\nmodel.\n3We conduct extensive evaluations using our base and chat models. The evaluation results\ndemonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particu-\nlarly in the \ufb01elds of code, mathematics, and reasoning. Following SFT and DPO, the DeepSeek\n67B chat model outperforms GPT-3.5 in both Chinese and English open-ended evaluations. This\nhighlights the superior performance of DeepSeek 67B in generating high-quality responses and\nengaging in meaningful conversations in both languages. Furthermore, the safety evaluation\nindicates that DeepSeek 67B Chat can provide harmless responses in practice.\nIn the rest of this paper, we \ufb01rst introduce our pre-training basic concepts of DeepSeek\nLLM in Section 2, including the composition of data, model architecture, infrastructure, and\nhyperparameters. In Section 3, we provide a detailed explanation of the scaling laws we have\ndiscovered and its implications. Additionally, we discuss the rationale behind our selection of\npre-training hyperparameters, taking into account the insights gained from the scaling laws\nanalysis. In Section 4, we discuss our \ufb01ne-tuning methodology, encompassing the composition\nof \ufb01ne-tuning data and speci\ufb01c methods during the SFT and DPO stages. We then present\nthe detailed evaluation results of DeepSeek LLM in Section 5, covering both the base and chat\nmodels, as well as their performance in open-ended evaluations and safety evaluations. Finally,\nwe discuss the current limitations and future directions of DeepSeek LLM in Section 6.\n2.Pre-Training\n2.1.Data\nOur main objective is to comprehensively enhance the richness and diversity of the dataset.\nWe have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al.,\n2020; Penedo et al., 2023; Touvron et al., 2023a). To achieve these goals, we have organized our\napproach into three essential stages: deduplication, \ufb01ltering, and remixing. The deduplication\nand remixing stages ensure a diverse representation of the data by sampling unique instances.\nThe \ufb01ltering stage enhances the density of information, thereby enabling more ef\ufb01cient and\neffective model training.\nWe adopted an aggressive deduplication strategy, expanding the deduplication scope. Our\nanalysis revealed that deduplicating the entire Common Crawl corpus results in higher removal\nof duplicate instances compared to deduplicating within a single dump. Table 1 illustrates\nthat deduplicating across 91 dumps eliminates four times more documents than a single dump\nmethod.\nDumps Used 1 2 6 12 16 22 41 91\nDeduplication Rate (%) 22.2 46.7 55.7 69.9 75.7 76.3 81.6 89.8\nTable 1 |Deduplication ratios for various Common Crawl dumps.\nIn the \ufb01ltering stage, we focus on developing robust criteria for document quality assess-\nment. This involves a detailed analysis incorporating both linguistic and semantic evaluations,\nproviding a view of data quality from individual and global perspectives. In the remixing phase,\nwe adjust our approach to address data imbalances, focusing on increasing the presence of\nunderrepresented domains. This adjustment aims to achieve a more balanced and inclusive\ndataset, ensuring that diverse perspectives and information are adequately represented.\nFor our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm\nbased on the tokenizers library (Huggingface Team, 2019). Pre-tokenization was employed to\n4\nFailed to extract the rating information.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\n# Open the PDF file\npdf_file = open('./data/2401.02954v1.pdf', 'rb')\npdf_reader = PyPDF2.PdfReader(pdf_file)\n\n# Extract the text from the PDF file\ntext = ''\nfor page in pdf_reader.pages:\n    text += page.extract_text()\n\n# Print the extracted text to see if the rating information is present\nprint(text)\n\n# Find the rating information\nrating_deepseek = None\nrating_llama2 = None\nfor line in text.split('\\n'):\n    if 'Deepseek' in line and 'Humaneval' in line:\n        rating_deepseek = float(line.split()[-1])\n    elif 'Llama2' in line and 'Humaneval' in line:\n        rating_llama2 = float(line.split()[-1])\n\n# Calculate the difference in ratings\nif rating_deepseek is not None and rating_llama2 is not None:\n    difference = rating_deepseek - rating_llama2\n    print(f'The rating of the Deepseek model is {rating_deepseek} and the rating of Llama2 is {rating_llama2}. The Deepseek model is {difference} higher than Llama2 on Humaneval.')\nelse:\n    print('Failed to extract the rating information.')\n\npdf_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<|assistant|>\n\nAnalyse: The code was unable to extract the rating information from the PDF file. The PDF file contains the text of the paper, but the code is not able to identify the rating information.\n\nThis Step Todo: Debug the code to see why it was unable to extract the rating information.\n\nCode:\n<|execute_start|>"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DeepSeek LLM\nScaling Open-Source Language Models with Longtermism\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,\nRuiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He,\nWenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang,\nFangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu,\nShanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu,\nTongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song,\nXuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang,\nShiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie,\nYiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu,\nXingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang,\nMinghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,\nShangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou*\n*DeepSeek-AI\nAbstract\nThe rapid development of open-source large language models (LLMs) has been truly remarkable.\nHowever, the scaling laws described in previous literature presents varying conclusions, which\ncasts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our\ndistinctive \ufb01ndings that facilitate the scaling of large scale models in two prevalent used open-\nsource con\ufb01gurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM,\na project dedicated to advancing open-source language models with a long-term perspective.\nTo support the pre-training phase, we have developed a dataset that currently consists of 2\ntrillion tokens and is continuously expanding. We further conduct supervised \ufb01ne-tuning (SFT)\nand direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM\n67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code,\nmathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek\nLLM 67B Chat exhibits superior performance compared to GPT-3.5.\n*Authors are ordered alphabetically by the last name.arXiv:2401.02954v1  [cs.CL]  5 Jan 2024Contents\n1 Introduction 3\n2 Pre-Training 4\n2.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.4 Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3 Scaling Laws 7\n3.1 Scaling Laws for Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Estimating Optimal Model and Data Scaling . . . . . . . . . . . . . . . . . . . . . 9\n3.3 Scaling Laws with Different Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4 Alignment 12\n5 Evaluation 13\n5.1 Public Benchmark Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n5.1.1 Base Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n5.1.2 Chat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n5.2 Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2.1 Chinese Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2.2 English Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.3 Held-Out Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.4 Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n6 Conclusion, Limitation, and Future Work 23\nA Appendix 30\nA.1 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.2 Different Model Scale Representations . . . . . . . . . . . . . . . . . . . . . . . . . 30\nA.3 Benchmark Metrics Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nA.4 Comparison with Code or Math Speci\ufb01c Models . . . . . . . . . . . . . . . . . . . 32\nA.5 Benchmark Results w/ DPO Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nA.6 Evaluation Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n21.Introduction\nOver the past few years, Large Language Models (LLMs) based on decoder-only Transformers\n(Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Arti-\n\ufb01cial General Intelligence (AGI). By predicting the next word in continuous text, LLMs undergo\nself-supervised pre-training on massive datasets, enabling them to achieve various purposes and\npossess many abilities, such as novel creation, text summarization, code completion, and more.\nSubsequent developments like supervised \ufb01ne-tuning and reward modeling have enabled Large\nLanguage Models (LLMs) to better follow user intentions and instructions. This has endowed\nthem with more versatile conversational capabilities and rapidly expanded their in\ufb02uence.\nThis wave is sparked with closed products , such as ChatGPT (OpenAI, 2022), Claude (An-\nthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational\nresources and substantial annotation costs. These products have signi\ufb01cantly raised the commu-\nnity\u2019s expectations for the capabilities of open-source LLMs, consequently inspiring a series of\nwork (Bai et al., 2023; Du et al., 2022; Jiang et al., 2023; Touvron et al., 2023a,b; Yang et al., 2023).\nAmong these, the LLaMA series models (Touvron et al., 2023a,b) stand out. It consolidates a\nrange of works to create an ef\ufb01cient and stable architecture, building well-performing models\nranging from 7B to 70B parameters. Consequently, the LLaMA series has become the de facto\nbenchmark for architecture and performance among open-source models.\nFollowing LLaMA, the open-source community has primarily focused on training \ufb01xed-size\n(7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM\nscaling laws (Hoffmann et al., 2022; Kaplan et al., 2020). Nonetheless, research on scaling laws is\nof utmost importance, considering that the current open-source models are merely at the initial\nstage of Arti\ufb01cial General Intelligence (AGI) development. In addition, early works (Hoffmann\net al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data\nwith increased compute budgets and inadequately addressed hyperparameter discussions. In\nthis paper, we extensively investigate the scaling behavior of language models and apply our\n\ufb01ndings in two widely used large-scale model con\ufb01gurations, namely 7B and 67B. Our study\naims to lay the groundwork for future scaling of open-source LLMs, paving the way for further\nadvancements in this domain. Speci\ufb01cally, we \ufb01rst examined the scaling laws of batch size\nand learning rate, and found their trends with model size. Building on this, we conducted a\ncomprehensive study of the scaling laws of the data and model scale, successfully revealing the\noptimal model/data scaling-up allocation strategy and predicting the expected performance\nof our large-scale models. Additionally, during development, we discovered that the scaling\nlaws derived from different datasets show signi\ufb01cant differences. This suggests that choice\nof dataset remarkably affects the scaling behavior, indicating that caution should be exercised\nwhen generalizing scaling laws across datasets.\nUnder the guidance of our scaling laws, we build from scratch open-source large language\nmodels, and release as much information as possible for community reference. We collect\n2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we\ngenerally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler\nwith a multi-step learning rate scheduler, maintaining performance while facilitating continual\ntraining. We collected over 1 million instances for supervised \ufb01ne-tuning (SFT) (Ouyang et al.,\n2022) from diverse sources. This paper shares our experiences with different SFT strategies\nand \ufb01ndings in data ablation techniques. Additionally, we have utilized direct preference\noptimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the\nmodel.\n3We conduct extensive evaluations using our base and chat models. The evaluation results\ndemonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particu-\nlarly in the \ufb01elds of code, mathematics, and reasoning. Following SFT and DPO, the DeepSeek\n67B chat model outperforms GPT-3.5 in both Chinese and English open-ended evaluations. This\nhighlights the superior performance of DeepSeek 67B in generating high-quality responses and\nengaging in meaningful conversations in both languages. Furthermore, the safety evaluation\nindicates that DeepSeek 67B Chat can provide harmless responses in practice.\nIn the rest of this paper, we \ufb01rst introduce our pre-training basic concepts of DeepSeek\nLLM in Section 2, including the composition of data, model architecture, infrastructure, and\nhyperparameters. In Section 3, we provide a detailed explanation of the scaling laws we have\ndiscovered and its implications. Additionally, we discuss the rationale behind our selection of\npre-training hyperparameters, taking into account the insights gained from the scaling laws\nanalysis. In Section 4, we discuss our \ufb01ne-tuning methodology, encompassing the composition\nof \ufb01ne-tuning data and speci\ufb01c methods during the SFT and DPO stages. We then present\nthe detailed evaluation results of DeepSeek LLM in Section 5, covering both the base and chat\nmodels, as well as their performance in open-ended evaluations and safety evaluations. Finally,\nwe discuss the current limitations and future directions of DeepSeek LLM in Section 6.\n2.Pre-Training\n2.1.Data\nOur main objective is to comprehensively enhance the richness and diversity of the dataset.\nWe have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al.,\n2020; Penedo et al., 2023; Touvron et al., 2023a). To achieve these goals, we have organized our\napproach into three essential stages: deduplication, \ufb01ltering, and remixing. The deduplication\nand remixing stages ensure a diverse representation of the data by sampling unique instances.\nThe \ufb01ltering stage enhances the density of information, thereby enabling more ef\ufb01cient and\neffective model training.\nWe adopted an aggressive deduplication strategy, expanding the deduplication scope. Our\nanalysis revealed that deduplicating the entire Common Crawl corpus results in higher removal\nof duplicate instances compared to deduplicating within a single dump. Table 1 illustrates\nthat deduplicating across 91 dumps eliminates four times more documents than a single dump\nmethod.\nDumps Used 1 2 6 12 16 22 41 91\nDeduplication Rate (%) 22.2 46.7 55.7 69.9 75.7 76.3 81.6 89.8\nTable 1 |Deduplication ratios for various Common Crawl dumps.\nIn the \ufb01ltering stage, we focus on developing robust criteria for document quality assess-\nment. This involves a detailed analysis incorporating both linguistic and semantic evaluations,\nproviding a view of data quality from individual and global perspectives. In the remixing phase,\nwe adjust our approach to address data imbalances, focusing on increasing the presence of\nunderrepresented domains. This adjustment aims to achieve a more balanced and inclusive\ndataset, ensuring that diverse perspectives and information are adequately represented.\nFor our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm\nbased on the tokenizers library (Huggingface Team, 2019). Pre-tokenization was employed to\n4\nFailed to extract the rating information.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import PyPDF2\n\n# Open the PDF file\npdf_file = open('./data/2401.02954v1.pdf', 'rb')\npdf_reader = PyPDF2.PdfReader(pdf_file)\n\n# Extract the text from the PDF file\ntext = ''\nfor page in pdf_reader.pages:\n    text += page.extract_text()\n\n# Print the extracted text to see if the rating information is present\nprint(text)\n\n# Find the rating information\nrating_deepseek = None\nrating_llama2 = None\nfor line in text.split('\\n'):\n    if 'Deepseek' in line and 'Humaneval' in line:\n        rating_deepseek = float(line.split()[-1])\n    elif 'Llama2' in line and 'Humaneval' in line:\n        rating_llama2 = float(line.split()[-1])\n\n# Calculate the difference in ratings\nif rating_deepseek is not None and rating_llama2 is not None:\n    difference = rating_deepseek - rating_llama2\n    print(f'The rating of the Deepseek model is {rating_deepseek} and the rating of Llama2 is {rating_llama2}. The Deepseek model is {difference} higher than Llama2 on Humaneval.')\nelse:\n    print('Failed to extract the rating information.')\n\npdf_file.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}